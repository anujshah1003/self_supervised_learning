{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-1: Split the data intro train and test (check the eda notebook to see the data).\n",
    "### Step-2: Take out 10% of the training data as a small labeled data to evaluate the efficiency of self-supervised learning.\n",
    "### Step-3: train on this small labeled data and compute the results on test data\n",
    "### Step-4: Do self-supervised learning on entire training data without using their labels\n",
    "### Step-5: use the pretrained rotnet model and fine tune on the small labeled data \n",
    "### Step-6: use the above finetuned model and evaluate the perfromance on test data.\n",
    "### Step-7: Compare the test results from step-3 with step-6 to see the benefits of unsupervised/self-supervised pretraining\n",
    "\n",
    "### --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) see the notebook preparing_labeled_train_test_csv_files.ipynb\n",
    "### b) 60 random flowers for each 5 flower category is taken as test data totaling to 300 test samples\n",
    "### c) The EDA for different flowers distribution in each train and test category is at EDA_Flowers.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) 10% of the train data is taken to be used as small labeled data.\n",
    "### b) This data is prepared by taking 10% of from each flower category.\n",
    "### c) The code for this is available at prepare_small_labeled_data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Prepare a dataloader for loading data\n",
    "### b) Preapare a config file\n",
    "### c) We will be using off-the-shelf ResNet-18 model\n",
    "### d) Train the model and record the results in logfile as well as tensorboard\n",
    "### e) Choose the best checkpoint and compute the result on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4 \n",
    "### a) prepare dataloader for rotnet\n",
    "### b) prepare config file for self-supervised learning\n",
    "### c) Train the model with self-supervised task/proxy labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5\n",
    "### a) Use the pretrained rotnet model by removing last layer and adding new layer for flower classification task\n",
    "### b) Freeze the old layers to preserve the feature learned during unsupervised pretraining\n",
    "### c) Fine tune the model for the flower classification task with small labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6\n",
    "### a)use the above finetuned model and evaluate the perfromance on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-7\n",
    "### a)Compare the test results from step-3 with step-6 to see the benefits of unsupervised/self-supervised pretraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
